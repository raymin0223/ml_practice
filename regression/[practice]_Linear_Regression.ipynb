{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic setting\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization of Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "with open('./data/linear_regression.pickle', 'rb') as f:\n",
    "    X, y = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the dataset distribution    \n",
    "plt.scatter(X, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [P.1] 데이터를 학습 데이터와 평가 데이터로 나누기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use sklearn library\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "from sklearn.model_selection import ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = ???(X, y, test_size=???, random_state=???)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize train dataset  \n",
    "plt.scatter(X_train, y_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize train dataset  \n",
    "plt.scatter(X_test, y_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [P.2] <font color=red>numpy</font> 라이브러리를 활용해, 최적의 선형 회귀 모델 찾기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 최적의 모델은 모든 데이터에 대해 실제값과 예측값의 차이가 제일 작은 모델\n",
    "- 선형 회귀 모델은  일반적으로 <font color=red>MSE(Mean Squared Error)</font> 손실을 최소화하도록 학습\n",
    "- 손실 함수와 모델 파라미터의 gradient에 관한 빈칸 부분 채워넣기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MSE 손실 함수**\n",
    "\\begin{equation*}\n",
    "\\left( \\frac{1}{n} \\sum_{i=1}^n (y_i - (wX_i + b))^2 \\right)\n",
    "\\end{equation*}\n",
    "\n",
    "**Gradients 계산**\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial L}{\\partial w} = -2 * \\frac{1}{n} \\sum_{i=1}^n (y_i - (wX_i + b)) * X_i\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial L}{\\partial b} = -2 * \\frac{1}{n} \\sum_{i=1}^n (y_i - (wX_i + b))\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gd_numpy(X, y, epochs, lr):\n",
    "    # Model weights and bias parameters\n",
    "    w = ???\n",
    "    b = ???\n",
    "\n",
    "    # Store model parameters and loss for visualization\n",
    "    w_list, b_list, loss_list = [], [], []\n",
    "\n",
    "    # Perform Gradient Descent\n",
    "    for i in range(???):\n",
    "\n",
    "\n",
    "    #################################################\n",
    "    ######## Hint: use +, -, *, /, **, np.mean() ########\n",
    "        loss = ???   # MSE loss\n",
    "\n",
    "        dw = ???   # derivative w.r.t to w\n",
    "        db = ???   # derivative w.r.t to b\n",
    "    #################################################\n",
    "\n",
    "\n",
    "        w = ???   # update w\n",
    "        b = ???   # update b\n",
    "\n",
    "        w_list.append(w)\n",
    "        b_list.append(b)\n",
    "        loss_list.append(loss)\n",
    "\n",
    "    print('Trained model weights : %.4f' % w)\n",
    "    print('Trained model bias : %.4f' % b)\n",
    "    \n",
    "    return w, b, w_list, b_list, loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setting\n",
    "epochs = ???\n",
    "learning_rate = ???\n",
    "\n",
    "w, b, w_list, b_list, loss_list = gd_numpy(X_train, y_train, epochs, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get train loss\n",
    "y_pred = ???\n",
    "\n",
    "train_loss = np.mean((y_train - y_pred)**2)\n",
    "print('Train Loss for LinearRegression model : %.4f' % train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the intermediate trained model\n",
    "nums = 6 \n",
    "epochs_list = [round(epochs / (nums-1) * n) for n in range(nums)]\n",
    "\n",
    "for i in range(len(epochs_list)):\n",
    "    plt.scatter(X_train, y_train)   # scatter the original data\n",
    "    \n",
    "    # Load trained weights in specific epoch\n",
    "    epoch = epochs_list[i] - 1   # In python, all indexes start from 0\n",
    "    w = w_list[epoch]\n",
    "    b = b_list[epoch]\n",
    "    \n",
    "    plt.plot(X_train, y_pred, color='red')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the change of loss\n",
    "plt.plot(loss_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 평가 데이터에 대한 예측 결과는 다음과 같음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the trained linear regression model\n",
    "# scatter the original data\n",
    "plt.scatter(X_test, y_test)   \n",
    "\n",
    "# plot prediction results\n",
    "y_pred = ???\n",
    "\n",
    "plt.plot(X_test, y_pred, color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습된 선형 회귀 모델의 성능은 평가 데이터셋에 대한 손실임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = np.mean((y_test - y_pred)**2)\n",
    "print('Test Loss for LinearRegression model : %.4f' % test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [P.3] <font color=red>sklearn</font> 라이브러리의 `LinearRegression` 모델을 활용해, 최적의 선형 회귀 모델 찾기\n",
    "\n",
    "- 이 모듈은 경사하강법 대신 최소제곱법(Least Squares Method)를 사용해 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_sklearn(X, y):\n",
    "    X_2d = X.???\n",
    "    \n",
    "    reg = ???\n",
    "    \n",
    "    w = ???\n",
    "    b = ???\n",
    "\n",
    "    print('Trained model weights : %.4f' % w)\n",
    "    print('Trained model bias : %.4f' % b)\n",
    "    \n",
    "    return reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lr_sklearn(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get train loss\n",
    "y_pred = model.???\n",
    "\n",
    "train_loss = np.mean((y_train - y_pred)**2)\n",
    "print('Train Loss for LinearRegression model : %.4f' % train_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 평가 데이터에 대한 예측 결과는 다음과 같이 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the trained linear regression model\n",
    "# scatter the original data\n",
    "plt.scatter(X_test, y_test)   \n",
    "\n",
    "# plot prediction results\n",
    "y_pred = model.???\n",
    "\n",
    "plt.plot(X_test, y_pred, color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get test loss\n",
    "test_loss = np.mean((y_test - y_pred)**2)\n",
    "print('Test Loss for LinearRegression model : %.4f' % test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [P.4] <font color=red>sklearn</font> 라이브러리의 `SGDRegression` 모델을 활용해, 최적의 선형 회귀 모델 찾기\n",
    "\n",
    "- 이 모듈은 경사하강법을 사용해 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html\n",
    "from sklearn.linear_model import SGDRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gd_sklearn(X, y, epochs, lr, alpha):\n",
    "    X_2d = X.???\n",
    "    \n",
    "    reg = SGDRegressor(penalty='l2', \n",
    "                       alpha=alpha,\n",
    "                       max_iter=epochs,\n",
    "                       tol=1e-3,\n",
    "                       learning_rate='invscaling',\n",
    "                       eta0=lr,\n",
    "                       random_state=42)\n",
    "    \n",
    "    reg.???\n",
    "    \n",
    "    w = reg.???\n",
    "    b = reg.???\n",
    "\n",
    "    print('Trained model weights : %.4f' % w)\n",
    "    print('Trained model bias : %.4f' % b)\n",
    "    \n",
    "    return reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training setting\n",
    "epochs = 1000\n",
    "lr = 1e-5\n",
    "alpha = 0.001\n",
    "\n",
    "model = gd_sklearn(X_train, y_train, epochs, lr, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get train loss\n",
    "y_pred = model.???\n",
    "\n",
    "train_loss = np.mean((y_train - y_pred)**2)\n",
    "print('Train Loss for LinearRegression model : %.4f' % train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the trained linear regression model\n",
    "# scatter the original data\n",
    "plt.scatter(X_test, y_test)   \n",
    "\n",
    "# plot prediction results\n",
    "y_pred = model.???\n",
    "\n",
    "plt.plot(X_test, y_pred, color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get test loss\n",
    "test_loss = np.mean((y_test - y_pred)**2)\n",
    "print('Test Loss for LinearRegression model : %.4f' % test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sklearn` 라이브러리를 사용하면, 별도의 gradient 계산 없이 모델을 쉽게 학습시킬 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
