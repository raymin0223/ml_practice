{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../')\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML 모델 학습\n",
    "\n",
    "지금까지 배웠던 모델들을 활용해서 이전의 `EDA` 파일에서 만들었던 정제된 데이터셋으로 학습 및 평가를 진행하면 됩니다.     \n",
    "집값 예측 데이터셋은 회귀 문제이므로, 회귀 모델들을 사용하면 됩니다.     \n",
    "\n",
    "Ensemble 자료에서 배웠던 모델들은 각각 `Classifier`에서 `Regressor`로 이름을 바꾸면, 회귀 모델이 되는 점 참고해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X dataset size: (1457, 220)\n",
      "y dataset size: (1457, 1)\n"
     ]
    }
   ],
   "source": [
    "X = pd.read_csv('./data/preprocessed_X.csv')\n",
    "y = pd.read_csv('./data/preprocessed_y.csv')\n",
    "\n",
    "print(\"X dataset size:\", X.shape)\n",
    "print(\"y dataset size:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "원하는 모델 구현해서 평가 데이터셋에 대한 성능을 찍고 분석해보세요.    \n",
    "이 때, 회귀 모델에 맞는 평가 지표를 `sklearn.metrics`에서 찾아서 사용해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sangmin/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDRegressor(alpha=0.01, eta0=0.001)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "reg = SGDRegressor(penalty='l2', \n",
    "                   alpha=0.01,\n",
    "                   max_iter=1000,\n",
    "                   tol=1e-3,\n",
    "                   learning_rate='invscaling',\n",
    "                   eta0=0.001)\n",
    "\n",
    "reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression model loss : 8758923202264192661020007727104.0000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "y_pred = reg.predict(X_test)\n",
    "loss = mean_squared_error(y_pred, y_test)\n",
    "print('Linear Regression model loss : %.4f' % loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sangmin/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVR(C=5, gamma=5)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "svr = SVR(kernel='rbf', gamma=5, C=5)\n",
    "\n",
    "svr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Vector Regression model loss : 0.1541\n"
     ]
    }
   ],
   "source": [
    "y_pred = svr.predict(X_test)\n",
    "loss = mean_squared_error(y_pred, y_test)\n",
    "print('Support Vector Regression model loss : %.4f' % loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=1.0 will be ignored. Current value: bagging_fraction=0.75\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMRegressor(bagging_fraction=0.75, bagging_freq=5, bagging_seed=7,\n",
       "              feature_fraction=0.2, feature_fraction_seed=7, learning_rate=0.01,\n",
       "              max_bin=200, n_estimators=5000, num_leaves=4,\n",
       "              objective='regression', verbose=-1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "lightgbm = LGBMRegressor(objective='regression', \n",
    "                         num_leaves=4,\n",
    "                         learning_rate=0.01, \n",
    "                         n_estimators=5000,\n",
    "                         max_bin=200, \n",
    "                         bagging_fraction=0.75,\n",
    "                         bagging_freq=5, \n",
    "                         bagging_seed=7,\n",
    "                         feature_fraction=0.2,\n",
    "                         feature_fraction_seed=7,\n",
    "                         verbose=-1,\n",
    "                         #min_data_in_leaf=2,\n",
    "                         #min_sum_hessian_in_leaf=11\n",
    "                         )\n",
    "\n",
    "lightgbm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM model loss : 0.0119\n"
     ]
    }
   ],
   "source": [
    "y_pred = lightgbm.predict(X_test)\n",
    "loss = mean_squared_error(y_pred, y_test)\n",
    "print('LightGBM model loss : %.4f' % loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=0.7, enable_categorical=False,\n",
       "             gamma=0, gpu_id=-1, importance_type=None,\n",
       "             interaction_constraints='', learning_rate=0.01, max_delta_step=0,\n",
       "             max_depth=3, min_child_weight=0, missing=nan,\n",
       "             monotone_constraints='()', n_estimators=3460, n_jobs=12,\n",
       "             num_parallel_tree=1, predictor='auto', random_state=27,\n",
       "             reg_alpha=6e-05, reg_lambda=1, scale_pos_weight=1, seed=27,\n",
       "             subsample=0.7, tree_method='exact', validate_parameters=1,\n",
       "             verbosity=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "xgboost = XGBRegressor(learning_rate=0.01, \n",
    "                       n_estimators=3460,\n",
    "                       max_depth=3, \n",
    "                       min_child_weight=0,\n",
    "                       gamma=0, \n",
    "                       subsample=0.7,\n",
    "                       colsample_bytree=0.7,\n",
    "                       scale_pos_weight=1, \n",
    "                       seed=27,\n",
    "                       reg_alpha=0.00006)\n",
    "                               \n",
    "\n",
    "xgboost.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost model loss : 0.0111\n"
     ]
    }
   ],
   "source": [
    "y_pred = xgboost.predict(X_test)\n",
    "loss = mean_squared_error(y_pred, y_test)\n",
    "print('XGBoost model loss : %.4f' % loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1. 어떤 데이터 전처리 과정을 진행했는지 정리해주세요.\n",
    "\n",
    "손실 데이터셋 제거, 아웃라이어 제거, 필요없는 피처 제거하거나 새로운 변수 생성, 몇몇 변수들의 값 정규화 등의 전처리들을 적용함.     \n",
    "또한 SalePrice 또한 로그 함수를 취해 값을 변화시킴."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2. 데이터 전처리릍 통해 데이터의 샘플 개수와 피처 차원은 어떻게 바뀌었나요?\n",
    "\n",
    "처음 데이터셋의 크기는 (1460, 80)의 크기 였음(SalePrice 타겟 피처 고려 없이).     \n",
    "전처리를 통해 데이터셋의 모양은 (1457, 220)으로 바뀜.     \n",
    "학습에 방해가 될 샘플 3개를 제거하고, 범주형 데이터를 처리하기 위해 dummy 피처로 변화시키는 과정에서 220개의 피처 개수로 증가함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3. 어떤 모델을 사용해서 데이터를 학습했으며, 모델의 장점과 단점을 설명해주세요.\n",
    "\n",
    "Ensemble 모델들 중 최신 모델인 LightGBM과 XGBoost 모델을 사용했고 꽤 좋은 성능을 얻음.     \n",
    "Boosting 알고리즘들로, 결정 트리 기반으로 구현되었음에도 꽤 좋은 성능들을 보여주는 알고리즘임. 또한, 학습 속도 개선과 ensemble의 효과를 높이고자 한 기술들이 추가된 모델들임."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
