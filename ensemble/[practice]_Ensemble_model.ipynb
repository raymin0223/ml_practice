{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic setting\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [P.1] Bagging 모델\n",
    "\n",
    "먼저, 결정 트리 기반의 bagging 알고리즘을 구현할 것임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_moons(n_samples=1000, noise=0.50)\n",
    "# get train, test dataset\n",
    "???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "결정 트리를 기반으로 하기 때문에 이전의 `DecisionTreeClassifier` 클래스를 [`BaggingClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html)로 감싸서 모델을 구현함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ??? import ???  # 결정 트리 분류 모델\n",
    "from ??? import ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_clf = ???\n",
    "\n",
    "# Train 데이터 학습\n",
    "???\n",
    "# Test 데이터 학습\n",
    "???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ??? import ???\n",
    "\n",
    "???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 성능을 단순한 결정 트리 모델 성능과 비교해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결정 트리 모델로만 성능을 얻어보기!\n",
    "########################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습된 두 모델의 시각화를 해보면 다음과 같음\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dataset(X, y, show=True):\n",
    "    plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"s\")\n",
    "    plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"^\")\n",
    "    plt.plot(X[:, 0][y==2], X[:, 1][y==2], \"o\")\n",
    "    \n",
    "    if show:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(clf, axes=None):\n",
    "    if axes is None:\n",
    "        x0 = np.linspace(-3, 4, 100)\n",
    "        x1 = np.linspace(-3, 4, 100)\n",
    "    else:\n",
    "        x0 = np.linspace(axes[0][0], axes[0][1], 100)\n",
    "        x1 = np.linspace(axes[1][0], axes[1][1], 100)\n",
    "        \n",
    "    \n",
    "    x0, x1 = np.meshgrid(x0, x1)\n",
    "    X_new = np.c_[x0.ravel(), x1.ravel()]\n",
    "    \n",
    "    y_pred = clf.predict(X_new).reshape(x0.shape)\n",
    "    \n",
    "    plt.contourf(x0, x1, y_pred, alpha=0.25)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "\n",
    "plot_dataset(X, y, False)\n",
    "plot_decision_boundary(tree_clf)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "\n",
    "plot_dataset(X, y, False)\n",
    "plot_decision_boundary(bag_clf)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "확실히 결정 경계가 더 단순해진 것을 통해, 여러 트리의 앙상블로 분산 오류가 줄어든 것을 볼 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [P.2] Random Forest 모델\n",
    "\n",
    "트리 모델 간의 상관 관계를 줄여 앙상블 효과를 극대화하는 Random Forest 모델을 구현하고 성능을 평가해볼 것임     \n",
    "사이킷런에서 제공하는 [`RandomForestClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) 클래스를 사용하면 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ??? import ???\n",
    "\n",
    "# Random Forest 모델을 정의하고 모델을 학습시켜보기\n",
    "#####################\n",
    "\n",
    "\n",
    "\n",
    "#####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "\n",
    "plot_dataset(X, y, False)\n",
    "plot_decision_boundary(rnd_clf)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging tree의 장점은 각 피처들의 중요도를 계산해, 모델에 대한 설명가능성을 키워줄 수 있음    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_clf.???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# iris 데이터셋 전체를 학습한 이후에, 각 피처에 대한 중요도를 뽑아주세요.\n",
    "# load_iris()['feature_names'] 활용\n",
    "####################\n",
    "\n",
    "\n",
    "\n",
    "####################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [P.3] AdaBoost 모델\n",
    "\n",
    "이번에는 Boosting 알고리즘인 `AdaBoost`를 활용해 모델을 구현해볼 것임     \n",
    "사이킷런에서 제공하는 [`AdaBoostClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html) 클래스를 사용하면 쉽게 구현 가능함     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ??? import ???\n",
    "\n",
    "ada_clf = ???\n",
    "\n",
    "# 이전과 똑같이 학습 및 평가해주세요.\n",
    "####################\n",
    "\n",
    "\n",
    "\n",
    "####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "\n",
    "plot_dataset(X, y, False)\n",
    "plot_decision_boundary(ada_clf)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Q1. Bagging과 Boosting의 차이가 무엇일까요?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2. AdaBoost에서 `DecisionTreeClassifier`의 `max_depth` argument를 1로 설정해준 이유는?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3. AdaBoost 모델이 만약 학습 데이터셋에 underfit된 상황이라면, 어떤 하이퍼파라미터를 사용할 수 있을까요?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "nav_menu": {
   "height": "252px",
   "width": "333px"
  },
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
